{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxJucrKj7mHJ"
   },
   "source": [
    "# Lab 2.1 - Weather Data Around Winona\n",
    "\n",
    "In this lab, we will download and combine a decades worth of weather data from the NOAA, focusing on weather stations within 500 miles of Winona.\n",
    "\n",
    "Here is the outline of the basic process.\n",
    "\n",
    "1. Install and investigate useful packages.\n",
    "2. Find all weather stations in proximity to Winona.\n",
    "3. Use a single station to prototype our tools.\n",
    "4. Automate the process of downloading and uncompressing data from all stations of interest.\n",
    "5. Output the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeicFuT-8Vnx"
   },
   "source": [
    "## Problem 1 - Install and investigate useful tools.\n",
    "\n",
    "First, you should install and investigate the following tools.\n",
    "\n",
    "1. **`wget`** is a tool for programmically downloading data files from the web on the command line.  There is a Python wrapper to this tool that you can install with `pip` as shown below.\n",
    "2. **`geopy`** is a package that, among other things, implements a function for computing distances between two lat-long pairs. Again, install this package with `pip` as shown below.\n",
    "3. **`gzip`** is part of the standard Python library and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c8pRv9PyCvPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\mp5667di\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uPWEksjS9REC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\mp5667di\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (2.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\mp5667di\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXgqoPb9e53"
   },
   "source": [
    "#### Task 1.1 - Investigate using `wget` to download a file.\n",
    "\n",
    "Read the help/documentation on `wget` to figure out how to download the following data file [Some random data file from STAT 210] into the `./data` sub-folder.\n",
    "\n",
    "[https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv](https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(wget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D92NkCq_995Z"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: ./data/sars1 (1).csv\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv\"\n",
    "output_path = \"./data/sars1.csv\"  # My file path\n",
    "\n",
    "filename = wget.download(url, out=output_path)\n",
    "\n",
    "print(f\"File downloaded to: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTW9hZXX-Ceq"
   },
   "source": [
    "#### Task 1.2 - Investigate using `geopy.distance.distance` to compute a distance in miles.\n",
    "\n",
    "1. Import the `distance` function from the `geopy.distance` submodule.\n",
    "2. Use Wikipedia to find the lat-long coordinates of Winona and Rochester MN.\n",
    "3. Use `distance` to compute the distance between Winona and Rochester.\n",
    "4. Use some other source (e.g., Google Maps) to check the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "m-XZZQ2v-i4t"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winona: 44.050556, -91.668333\n",
    "# Rochester: 44.023333, -92.461389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.54418575388878\n"
     ]
    }
   ],
   "source": [
    "winona = (44.050556, -91.668333)\n",
    "rochester = (44.023333, -92.461389)\n",
    "\n",
    "print(distance.distance(winona, rochester).miles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHdoE7ZFBXlU"
   },
   "source": [
    "#### Task 1.3 - Investigate `gzip`\n",
    "\n",
    "The yearly NOAA data is compressed as `.gz` files, which need to be uncompressed using `gzip`.  Explore the `gzip` module by\n",
    "\n",
    "1. Exploring the documentation/help for the `gzip` module,\n",
    "2. Using `wget` to download the following link into the `./data` folder, and\n",
    "3. Using `gzip` to uncompress this file.\n",
    "4. Inspect the data in your list, which should be of type `byte`.  Use a comprehension with the expression `l.decode('utf-8')` to convert this to a list of strings.\n",
    "5. Write the uncompressed lines to an output file using `with open(path, 'w') as out` and the `writelines` method of `out`.  \n",
    "\n",
    "**Link.** [https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz](https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "88NVuihyCBBO"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This command is not supported by the help utility.  Try \"(gzip) /?\".\n"
     ]
    }
   ],
   "source": [
    "!help(gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: ./data/1750.csv (1).gz\n"
     ]
    }
   ],
   "source": [
    "# Download the file into the ./data folder\n",
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz\"\n",
    "output_path = \"./data/1750.csv.gz\"  # My file path\n",
    "\n",
    "filename = wget.download(url, out=output_path)\n",
    "\n",
    "print(f\"File downloaded to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed file saved as: ./data/1750_uncompressed.csv\n"
     ]
    }
   ],
   "source": [
    "# Define input and output file paths\n",
    "file_to_uncompress = './data/1750.csv.gz'\n",
    "output_file = './data/1750_uncompressed.csv'\n",
    "\n",
    "# Step 1: Read the compressed file as bytes\n",
    "# .gz file is a binary file. \n",
    "# So, you have to use 'rb' instead of 'r'\n",
    "with open(file_to_uncompress, 'rb') as f: \n",
    "    compressed_data = f.read()  # Read the entire file in binary mode\n",
    "\n",
    "# Step 2: Decompress the data\n",
    "decompressed_data = gzip.decompress(compressed_data)  # Returns bytes\n",
    "\n",
    "# Step 3: Convert bytes to string and split into lines\n",
    "string_lines = decompressed_data.decode('utf-8').splitlines(keepends=True)\n",
    "\n",
    "# Step 4: Write the uncompressed lines to an output file\n",
    "with open(output_file, 'w') as out:\n",
    "    out.writelines(string_lines)\n",
    "\n",
    "print(f\"Uncompressed file saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt1MlIz9DWpm"
   },
   "source": [
    "## Problem 2 - Find all stations within 500 miles of Winona, MN.\n",
    "\n",
    "The file linked below contains information about all stations tracked by NOAA.  \n",
    "\n",
    "*Main folder:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/\n",
    "\n",
    "*Station txt file:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
    "\n",
    "*Note.* While it would be easier to use the CSV version of the station file, you should use the TXT version here (for practice).\n",
    "\n",
    "**Your tasks** Our goal is to get a list of stations that are within 500 miles of Winona.  Do this by\n",
    "\n",
    "1. Using `wget` to download the stations information into the `./data` folder.\n",
    "2. Use `with` to read the lines of this file.\n",
    "3. At this point, the lines are strings in a fixed-width format separated by whitespace.  Use a list comprehension with the string split method to split the raw lines (strings) into a list of entries.\n",
    "4. There are three entries of interest, the station ID and the lat-long coordinates of the station.  Inspect the file to determine the index for these three entries.\n",
    "5. We want to transform the lines (currently a list of strings) into a record, which is a `dict` with good names for the entries as keys and the values representing the data in an appropriate type (string for station ID, `float` for the lat-long).  Use a comprehension to create a list of records as described.\n",
    "6. Use another comprehension to apply a filter to the stations, keeping only those within 500 miles of Winona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "F0J443KoFTrs"
   },
   "outputs": [],
   "source": [
    "# Your code here (add cells as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: ./data/ghcnd-stations (1).txt\n"
     ]
    }
   ],
   "source": [
    "# Download the file into the ./data folder\n",
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "output_path = \"./data/ghcnd-stations.txt\"  # My file path\n",
    "\n",
    "filename = wget.download(url, out=output_path)\n",
    "\n",
    "print(f\"File downloaded to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       \\n',\n",
       " 'ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    \\n',\n",
       " 'AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\\n',\n",
       " 'AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194\\n',\n",
       " 'AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/ghcnd-stations.txt') as f:\n",
    "    lines = f.readlines()\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ACW00011604',\n",
       "  '17.1167',\n",
       "  '-61.7833',\n",
       "  '10.1',\n",
       "  'ST',\n",
       "  'JOHNS',\n",
       "  'COOLIDGE',\n",
       "  'FLD'],\n",
       " ['ACW00011647', '17.1333', '-61.7833', '19.2', 'ST', 'JOHNS'],\n",
       " ['AE000041196',\n",
       "  '25.3330',\n",
       "  '55.5170',\n",
       "  '34.0',\n",
       "  'SHARJAH',\n",
       "  'INTER.',\n",
       "  'AIRP',\n",
       "  'GSN',\n",
       "  '41196']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, split() handles multiple consecutive whitespace characters as a single delimiter.\n",
    "split_lines = [line.split() for line in lines]\n",
    "split_lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'station_id': 'ACW00011604', 'latitude': 17.1167, 'longitude': -61.7833}, {'station_id': 'ACW00011647', 'latitude': 17.1333, 'longitude': -61.7833}, {'station_id': 'AE000041196', 'latitude': 25.333, 'longitude': 55.517}]\n"
     ]
    }
   ],
   "source": [
    "# List comprehension to create records (dicts) with keys: 'station_id', 'latitude', 'longitude'\n",
    "stations = [\n",
    "            {'station_id': line[0],\n",
    "            'latitude': float(line[1]),  # Convert to float for latitude\n",
    "            'longitude': float(line[2])  # Convert to float for longitude\n",
    "            }\n",
    "    for line in split_lines]\n",
    "\n",
    "# Print the first 3 records\n",
    "print(stations[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rint(distance.distance(winona, rochester).miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'station_id': 'CA005012710', 'latitude': 49.45, 'longitude': -98.6167}, {'station_id': 'CA005020036', 'latitude': 49.55, 'longitude': -98.2}, {'station_id': 'CA005020040', 'latitude': 49.1, 'longitude': -97.55}]\n"
     ]
    }
   ],
   "source": [
    "# Winona coordinates\n",
    "winona = (44.050556, -91.668333)\n",
    "\n",
    "# Filter stations within 500 miles of Winona\n",
    "nearby_stations = [\n",
    "                    station for station in stations\n",
    "                    if distance.distance(winona, (station['latitude'], station['longitude'])).miles <= 500\n",
    "                  ]\n",
    "\n",
    "# Print filtered stations\n",
    "print(nearby_stations[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_9ve26IduT"
   },
   "source": [
    "#### Problem 3 - Prototype downloading and uncompressing a station file.\n",
    "\n",
    "Before we download and uncompress all the stations of interest, let's practice on one station file.\n",
    "\n",
    "\n",
    "1. Copy the url for some station and store is as a variable named `url`.\n",
    "2. Write `lambda` functions that extract each of the following from the station `url`: compressed file name, compressed file path (e.g., `./data/...`), and uncompressed file path (e.g., `./data/...`).\n",
    "3. Write a `lambda` function that extracts\n",
    "4. Use `wget` to download this stations data.\n",
    "5. Use `gzip` to uncompress the data.\n",
    "6. Write the data to out output file.\n",
    "\n",
    "Your code should have the following shape:\n",
    "\n",
    "```{Python}\n",
    "wget.download(...)\n",
    "with gzip.open(...) as f:\n",
    "    with open(..., 'w') as out:\n",
    "        f.readlines()\n",
    "        out.writelines(f)\n",
    "```\n",
    "\n",
    "You should be using your helper functions to, in part, fill in the `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "04YS60A1JciS"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Lambda function to add two numbers\n",
    "add_lambda = lambda x, y: x + y\n",
    "\n",
    "# Usage\n",
    "print(add_lambda(3, 5))  # Output: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACW00011604.csv.gz\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/ACW00011604.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACW00011604.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# lambda to extract compressed file name\n",
    "\n",
    "extract_comp_file_name = lambda url: url.split('/')[-1]\n",
    "print(extract_comp_file_name(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ghcn/daily/by_station/ACW00011604.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# lambda to extract compressed file path\n",
    "\n",
    "extract_comp_file_path = lambda url: '.' + url.split('pub')[1]#.replace('/', '\\\\')\n",
    "print(extract_comp_file_path(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/ghcn/daily/by_station/ACW00011604.csv\n"
     ]
    }
   ],
   "source": [
    "# lambda to extract uncompressed file path\n",
    "\n",
    "extract_uncomp_file_path = lambda url: '.' + url.split('pub')[1].replace('.gz', '')#.replace('/', '\\\\')\n",
    "print(extract_uncomp_file_path(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed file downloaded to: ./data/ghcn/daily/by_station/ACW00011604.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ACW00011604.csv.gz\n",
    "url = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/ACW00011604.csv.gz'\n",
    "\n",
    "# Download the compressed file\n",
    "compressed_file_path = extract_comp_file_path(url)\n",
    "uncompressed_file_path = extract_uncomp_file_path(url)\n",
    "\n",
    "# Ensure the directory exists before downloading or writing a file.\n",
    "os.makedirs(os.path.dirname(compressed_file_path), exist_ok=True)\n",
    "\n",
    "wget.download(url, compressed_file_path)\n",
    "\n",
    "# Uncompress the file and write to an output file\n",
    "with gzip.open(compressed_file_path, 'rt') as f:\n",
    "    with open(uncompressed_file_path, 'w') as out:\n",
    "        out.writelines(f.readlines())\n",
    "\n",
    "print(f\"Uncompressed file downloaded to: {uncompressed_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(os.makedirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzgFqv5VF38i"
   },
   "source": [
    "## Problem 4 - Build the station URLs and download the files.\n",
    "\n",
    "**Tasks.** Now you need to build urls for all stations of interest by\n",
    "\n",
    "1. Use a comprehension to extract the stations of interest into a list.\n",
    "2. Investigating the structure of the files stored in the `by_station` folder (see main folder link above).\n",
    "3. Use a comprehension and an `f` string to build a list of URLS for all stations of interest.\n",
    "4. Use `wget` to download the data for the stations of interest into the data folder.\n",
    "5. Use `gzip` to uncompress the files.\n",
    "6. Convert the `bytes` to `str` of format `utf-8`.\n",
    "7. Use the append mode `\"a\"` of `open` with `writelines` to append the data in each file to your output file.\n",
    "\n",
    "While we usually avoid using a `for` loop, we make an exception for code for lengthy IO.  To accomplish steps 4 & 5, use a `for` loop with the following shape.\n",
    "\n",
    "```{Python}\n",
    "for url in station_urls:\n",
    "    wget.download(...)\n",
    "    with gzip.open(...) as f:\n",
    "        with open(..., 'a') as out:\n",
    "            f.readlines()\n",
    "            ... # Convert lines to strings here\n",
    "            out.writelines(f)\n",
    "    print(f\"Downloaded and extracted the data for {url}\")\n",
    "```\n",
    "\n",
    "Note that the code inside the loop should resemble the code from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOUt7rCBIZ6f"
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tnGWOC1xF9kT",
    "outputId": "9b060cfe-aa8e-4930-880c-b720a36fc911"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://my_fake_website.cool/A123456789'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_station = \"A123456789\"\n",
    "make_fake_url = lambda s: f\"https://my_fake_website.cool/{s}\"\n",
    "\n",
    "make_fake_url(fake_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RPvTJEJGYtc",
    "outputId": "da643b74-66a9-4b29-8cce-a679a2d9fa5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://my_fake_website.cool/A0',\n",
       " 'https://my_fake_website.cool/A1',\n",
       " 'https://my_fake_website.cool/A2',\n",
       " 'https://my_fake_website.cool/A3',\n",
       " 'https://my_fake_website.cool/A4',\n",
       " 'https://my_fake_website.cool/A5',\n",
       " 'https://my_fake_website.cool/A6',\n",
       " 'https://my_fake_website.cool/A7',\n",
       " 'https://my_fake_website.cool/A8',\n",
       " 'https://my_fake_website.cool/A9']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_fake_stations =[f'A{i}' for i in range(10)]\n",
    "\n",
    "(my_fake_urls := [make_fake_url(s) for s in my_fake_stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVG4xpnvMvRK"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
